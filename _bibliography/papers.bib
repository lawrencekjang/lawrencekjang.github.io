---
---
@misc{dechezelles2024browsergymecosystemwebagent,
      title={The BrowserGym Ecosystem for Web Agent Research}, 
      author={Thibault Le Sellier De Chezelles and Maxime Gasse and Alexandre Drouin and Massimo Caccia and Léo Boisvert and Megh Thakkar and Tom Marty and Rim Assouel and Sahar Omidi Shayegan and Lawrence Keunho Jang and Xing Han Lù and Ori Yoran and Dehan Kong and Frank F. Xu and Siva Reddy and Quentin Cappart and Graham Neubig and Ruslan Salakhutdinov and Nicolas Chapados and Alexandre Lacoste},
      year={2024},
      eprint={2412.05467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05467}, 
      selected = {true},
      code = {https://github.com/ServiceNow/BrowserGym},
      preview={bgym.png}
}


@misc{jang2024videowebarenaevaluatinglongcontext,
      title={VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks}, 
      author={Lawrence Jang and Yinheng Li and Charles Ding and Justin Lin and Paul Pu Liang and Dan Zhao and Rogerio Bonatti and Kazuhito Koishida},
      year={2024},
      eprint={2410.19100},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.19100}, 
      website={https://videowebarena.github.io},
      code = {https://github.com/ljang0/videowebarena},
      selected = {true},
      preview={headerFINAL.png}
}


@misc{bonatti2024windowsagentarenaevaluating,
      title={Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale}, 
      author={Rogerio Bonatti and Dan Zhao and Francesco Bonacci and Dillon Dupont and Sara Abdali and Yinheng Li and Yadong Lu and Justin Wagle and Kazuhito Koishida and Arthur Bucker and Lawrence Jang and Zack Hui},
      year={2024},
      eprint={2409.08264},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      pdf={waaPaper.pdf},
      preview={waa.png},
      selected = {true},
      code={https://github.com/microsoft/WindowsAgentArena},
      website={https://microsoft.github.io/WindowsAgentArena/},
      url={https://arxiv.org/abs/2409.08264}}
}


@inproceedings{sarch2024ical,
                        title = "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights",
                        author = "Sarch, Gabriel and
                        Jang, Lawrence and
                        Tarr, Michael and
                        Cohen, William and
                        Marino, Kenneth and
                        Fragkiadaki, Katerina",
                        preview={ical.png},
                        booktitle = "",
                        website={https://ical-learning.github.io/},
                        code={https://github.com/Gabesarch/ICAL},
                        pdf={ical.pdf},
                        year = "2024",
                        selected={true}}

@misc{yu2024mmoeenhancingmultimodalmodels,
      title={MMoE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts}, 
      author={Haofei Yu and Zhengyang Qi and Lawrence Jang and Ruslan Salakhutdinov and Louis-Philippe Morency and Paul Pu Liang},
      year={2024},
      eprint={2311.09580},
      archivePrefix={arXiv},
      pdf={mmoe.pdf},
      preview={mmoe.png},
      primaryClass={cs.CL},
      selected = {true},
      url={https://arxiv.org/abs/2311.09580}, 
}

@inproceedings{koh-etal-2024-visualwebarena,
    title = "{V}isual{W}eb{A}rena: Evaluating Multimodal Agents on Realistic Visual Web Tasks",
    author = "Koh, Jing Yu  and
      Lo, Robert  and
      Jang, Lawrence  and
      Duvvur, Vikram  and
      Lim, Ming  and
      Huang, Po-Yu  and
      Neubig, Graham  and
      Zhou, Shuyan  and
      Salakhutdinov, Russ  and
      Fried, Daniel",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.50",
    pages = "881--905",
    pdf={vwa.pdf},
    preview={vwaNEW.png},
    website={https://jykoh.com/vwa},
    code={https://github.com/web-arena-x/visualwebarena},
    selected = {true},
    abstract = "Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.",
}
